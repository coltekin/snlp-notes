\chapter{Introduction}
%TODO: A good starting story, potentially involving history of NLP
%Potential resources:
% - https://blog.monkeylearn.com/the-definitive-guide-to-natural-language-processing/
%
%
% What is left for new NLP researchers/practitioners?
% (or, did google do all that can be done for NLP?)
%
% - Many languages, still few tools for majority

This is the accompanying notes
for an introductory undergraduate course on
\emph{statistical natural language processing}.
In my opinion, the modifier \emph{statistical} in the title of the course,
and these notes, is unnecessary.
The natural language processing (NLP) is necessarily statistical.
Of course we use many non-statistical components or algorithms,
but statistics (or machine learning) is a necessary component of any NLP system.
As a result, this course is a first-introduction to major concepts and applications of natural language processing.

There are already very good textbooks \parencite[notably][]{jurafsky2009,manning1999} on NLP.
This notes do not intend to replace them.
The notes follow the topics taught in the class,
and some topics discussed require other sources.
The main motivation is to provide students concise material for the students.
Having these notes also relives the instructor of trying to provide
slides that are instructive, and allows them to be more demonstrative.

\section{The plan}

The course will covers the following NLP topics and applications:

\begin{itemize}
  \item Tokenization, normalization
  \item Language models
  \item Part of speech tagging
  \item Morphology, morphological processing
  \item Probabilistic parsing
  \item Computational (vector space) semantics
  \item Word sense disambiguation
  \item Text categorization
  \item Question answering
  \item Summarization (?)
  \item Named entity recognition
  \item Information Retrieval
  \item Coreference resolution (?)
  \item Speech recognition (?)
  \item Grammar induction (?)
  \item Computational methods for linguistics and cognitive science (?)
  \item Corpora, corpus collection, annotation (?)
\end{itemize}

To be able to do get a fair understanding of these topics,
we will need some topics from mathematics,
probability theory, statistics, machine learning and linguistics.
These will be introduced (necessarily) rather superficially,
yet the introduction will be sufficient for the purposes of the course,
for most students.
Further pointers to books and resources will be provided for those
who need more material or want to learn these topic more in-depth.

\begin{itemize}
  \item Linguistic essentials
  \item Mathematical preliminaries
  \item Probability theory
  \item Statistics
  \item Information theory
  \item Machine learning basics: naive Bayes, regressing, logistic regression
  \item Bayesian learning
  \item Neural Networks
  \item Unsupervised learning
  \item The EM algorithm
  \item Unsupervised Bayesian learning
\end{itemize}
