\chapter{Tokenization, Segmentation, Normalization}

Almost any natural language processing application,
we use processing units such as sentences, words, morphemes.
These units, or linguistics objects,
are rarely available in clearly distinguishable form
even in well edited, high-quality text.
Identifying these units in the language data to be processed becomes
more difficult if one needs to process non-standard text or spoken language.
Furthermore,
different languages present different challenges.
For example,
some languages, such as Chinese and Japanese,
do not mark word boundaries in the written text,
which makes identifying words difficult.

These \emph{text processing} steps are essential for NLP systems.
Since the units identified in these stages are
input to the later stages of processing,
and any error at this stage will affect the usefulness of the NLP system.

\section{Document preprocessing}

\section{Tokenization}

\section{Segmentation}

\section{Normalization}

